# ETL-процесс

## Содержание проекта
* Директории:
1. AIRFLOW - содержит скрипт анализа качества данных, DAG на запуск поверки качества и DAG на осуществление процедуры select-а.
2. POSTGRES - содержит DDL-скрипт, предназначенный для формирования всех сущностей DDS-слоя.
* Files:
1. AIRFLOW/data_quality.py - python-скрипт с классом для проверки качества данных и загрузки данных из одной базы данных в другую.
2. AIRFLOW/upload_data_to_dds.py - python-скрипт с DAG-ом, запускающим процедуру проверки качества данных.
3. AIRFLOW/select_data_dag.py - python-скрипт с DAG-ом, запускающим процедуру возвращения заданного количества строк из целевой таблицы.
4. AIRFLOW/variables.txt - файл, с данными об используюшихся в AIRFLOW переменных.
5. POSTGRES/DDL.sql - SQL-скрипт, с процедурами создания сущностей на слое DDS

## Хранилище данных.
    Хранилище данных представлено четырьмя слоями: слой управления данных, слой временного хранения данных,
слой подготовленных данных, слой витрин данных.
    В слое управления данным указаны технологии, осуществляющие обработку данных. Docker осуществляет контейнеризацию Airflow с Python.
При этом, Airflow оркестрирует исполнение двух Python-скриптов, один из которых предназначен для трансформации и очистки исходных
данных и их последующую загрузку в схему подготовленных данных операционной базы данных. Далее, второй python-скрипт на основе полученных
данных формирует и загружает витрины данных. Образованные витрины данных визуализируются с помощью инструмента создания чартов и
дашбордов Superset, который, в свою очередь, контейнеризирован с помощью Docker.


![Схема архитектурного решения](https://github.com/asetimankulov/internship/assets/98170451/113a49fe-1646-446c-980b-fa714f21d505)
